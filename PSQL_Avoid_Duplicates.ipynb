{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225a006a",
   "metadata": {},
   "source": [
    "# `Avoiding Duplicate Entries PSQL with Python (PSYCOPG2)`\n",
    "\n",
    "# <font color=red>Mr Fugu Data Science</font>\n",
    "\n",
    "# (◕‿◕✿)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "665f8bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Using cached psycopg2-2.9.6.tar.gz (383 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: psycopg2\n",
      "  Building wheel for psycopg2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psycopg2: filename=psycopg2-2.9.6-cp310-cp310-macosx_10_9_universal2.whl size=144642 sha256=059479152484e6e430b378b0450d031b8bdb16dee2b4b5d261888bec5a4c827e\n",
      "  Stored in directory: /Users/zatoichi59/Library/Caches/pip/wheels/a2/65/83/78e6f42d3b8e22115e894576b71799d96ab5a790b8f7bcfa85\n",
      "Successfully built psycopg2\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.6\n"
     ]
    }
   ],
   "source": [
    "# Verify Psycog2 Install Unblock this if you don't have PSYCOPG2\n",
    "\n",
    "# !pip3 install pymongo\n",
    "!pip3 install psycopg2\n",
    "import psycopg2             # python->psql connection\n",
    "# import psycopg2.extras\n",
    "\n",
    "# import pandas as pd         # create dataframes \n",
    "# import os                   # fetch files\n",
    "\n",
    "\n",
    "# Import the 'config' function from the config_user_dta.py file:\n",
    "# from config_user_dta import config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1dfd22",
   "metadata": {},
   "source": [
    "# `You will need to create this file like me`\n",
    "\n",
    "`from config_user_dta import config`\n",
    "\n",
    "here is a link to do so: [My Github](https://github.com/MrFuguDataScience/SendDataFrom-R-Python-to-SQL-PSQL-Mongo/blob/master/HOW%20TO%20SEND%20DATA%20from%20Python%20to%20PSQL.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc7424a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61f2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d8d1072",
   "metadata": {},
   "source": [
    "# `lets start from the INSERT statement`\n",
    "\n",
    "When you decide to insert either a batch file, numberous CSV files or just a single line there are distinctions to consider for `avoiding duplicate entries`\n",
    "\n",
    "We have a few options depending on what scenario you fall into:\n",
    "\n",
    "1.) Is this a situation where you want to find duplicate entries you **`already have in an existing DB?`**\n",
    "\n",
    "+ **`DISTINCT`** keyword\n",
    "\n",
    "+ **`GROUPBY`** clause\n",
    "\n",
    "+ **`INNER JOIN`** two or more tables\n",
    "\n",
    "+ **`&&`** which is an `overlap` operator\n",
    "\n",
    "2.) Are you trying to **`prevent creating duplicate entries before adding`** to a database?\n",
    "\n",
    "\n",
    "\n",
    "+ **`ON CONFLICT DO NOTHING`**\n",
    "\n",
    "+ **`UNIQUE`** constraint\n",
    "\n",
    "+ Option to build a **`trigger/function`**\n",
    "\n",
    "`----------------------------------------------`\n",
    "\n",
    "+ **`Performance Issues:`**\n",
    "\n",
    "When using the `ON CONFLICT` you are able doing pre-checks are done to find conflicts before insertion. If there is a pass then insert is performed otherswise deleted attempt and moves on.\n",
    "\n",
    "+ This is a step in the right direction because you are avoiding overhead resources creating a heap that is later deleted. When this occurs you are creating dead tuples which create wasted storage space.\n",
    "\n",
    "\n",
    "+ `BLOAT:` when we are scanning tables and updating old entries with new ones we create dead tuples that occur from the deletion of an old entry \"tuple\". Overtime this will affect speed and storage space if it is not controled\n",
    "\n",
    "\n",
    "\n",
    "`----------------------------------------`\n",
    "\n",
    "**`ON CONFLICT DO NOTHING`**\n",
    "\n",
    "+ When a duplicate row is trying to be added it will be ignored and NOT create `bloat` such as dead rows of data and wasted space while creating an `insert`.\n",
    "+ `Consideration for UPDATES` you are deleting an old row and inserting a new row creating a dead row from the old deleted row during update which can cause bloat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67563b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider a staging \"temp\" table when doing `bulk insert` to check for duplicates or other stuff that may arise\n",
    "\n",
    "# check when \"On Conflict\" was introduced? maybe 9.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Examples and determine if I do with terminal, psql -> psycopg or just illustrations\n",
    "\n",
    "# Pros & Cons for each method...\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f325d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ON CONFLICT UPDATE, you can use SET to deal with columns also..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2331061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b8ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e24db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d533c4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bd902d6",
   "metadata": {},
   "source": [
    "# Like, Share & <font color=red>SUB</font>scribe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7d1f2",
   "metadata": {},
   "source": [
    "# `Citations`\n",
    "\n",
    "# ◔̯◔\n",
    "\n",
    "https://tomcam.github.io/postgres/\n",
    "\n",
    "https://aws.amazon.com/blogs/database/hidden-dangers-of-duplicate-key-violations-in-postgresql-and-how-to-avoid-them/\n",
    "\n",
    "https://www.freecodecamp.org/news/how-to-remove-duplicate-data-in-sql/#:~:text=One%20of%20the%20easiest%20ways,values%20from%20a%20particular%20column\n",
    "\n",
    "https://subscription.packtpub.com/book/data/9781803248974/5/ch05lvl1sec63/preventing-duplicate-rows\n",
    "\n",
    "https://stackoverflow.com/questions/67616081/preventing-insert-on-duplicate-values-postgres\n",
    "\n",
    "https://stackoverflow.com/questions/1109061/insert-on-duplicate-update-in-postgresql/30118648#30118648\n",
    "\n",
    "https://www.postgresql.org/docs/current/btree-gist.html\n",
    "\n",
    "https://codingsight.com/sql-insert-into-select-5-easy-ways-to-handle-duplicates/\n",
    "\n",
    "https://learn.microsoft.com/en-us/troubleshoot/sql/database-engine/development/remove-duplicate-rows-sql-server-tab\n",
    "\n",
    "https://www.mongodb.com/community/forums/t/batch-insert-upsert-avoiding-duplicates/163725 (mongodb)\n",
    "\n",
    "https://stackoverflow.com/questions/53722405/how-to-insert-bulk-rows-and-ignore-duplicates-in-postgresql-9-3\n",
    "\n",
    "https://www.psycopg.org/psycopg3/docs/advanced/async.html\n",
    "\n",
    "https://www.postgresql.org/docs/current/ddl-generated-columns.html\n",
    "\n",
    "https://www.appsloveworld.com/postgresql/100/58/how-to-do-a-bulk-insert-while-avoiding-duplicates-in-postgresql\n",
    "\n",
    "https://alibaba-cloud.medium.com/use-of-the-postgresql-upsert-insert-on-conflict-do-function-f366ac8afd52 (good examples)\n",
    "\n",
    "https://www.postgresqltutorial.com/postgresql-tutorial/how-to-delete-duplicate-rows-in-postgresql/\n",
    "\n",
    "https://www.delftstack.com/howto/postgres/postgresql-insert-on-duplicate-update/ (cool read, look at RACE ex.)\n",
    "\n",
    "`Bulk Insert`\n",
    "\n",
    "https://www.enterprisedb.com/blog/7-best-practice-tips-postgresql-bulk-data-loading\n",
    "\n",
    "https://www.commandprompt.com/education/how-to-insert-bulk-data-in-postgresql/\n",
    "\n",
    "https://www.sqlshack.com/working-with-line-numbers-and-errors-using-bulk-insert/\n",
    "\n",
    "https://www.2ndquadrant.com/en/blog/7-best-practice-tips-for-postgresql-bulk-data-loading/\n",
    "\n",
    "https://www.cockroachlabs.com/docs/stable/performance-best-practices-overview\n",
    "\n",
    "https://www.highgo.ca/2020/12/08/bulk-loading-into-postgresql-options-and-comparison/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225a006a",
   "metadata": {},
   "source": [
    "# `Avoiding Duplicate Entries PSQL`\n",
    "\n",
    "# <font color=red>Mr Fugu Data Science</font>\n",
    "\n",
    "# (◕‿◕✿)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d1072",
   "metadata": {},
   "source": [
    "# `Lets start from the` <font color=red>INSERT</font> `statement: you have 2 options`\n",
    "\n",
    "When you decide to `insert` either a *`batch file, numberous CSV files or just a single line`* there are distinctions to consider for `avoiding duplicate entries`\n",
    "\n",
    "`We have a few options depending on what scenario you fall into:`\n",
    "\n",
    "`--------------------------------------------`\n",
    "\n",
    "**`1. )`** This a situation where you want to find duplicate entries with an **`already have in an existing DB?`**\n",
    "\n",
    "+ **`DISTINCT`** keyword\n",
    "\n",
    "+ **`GROUPBY`** clause\n",
    "\n",
    "+ **`INNER JOIN`** two or more tables\n",
    "\n",
    "+ **`&&`** which is an `overlap` operator\n",
    "\n",
    "**`2. )`** Are you trying to **`prevent creating duplicate entries before adding`** to a database?\n",
    "\n",
    "\n",
    "\n",
    "+ **`ON CONFLICT DO NOTHING`**\n",
    "\n",
    "+ **`UNIQUE`** constraint\n",
    "\n",
    "+ Option to build a **`trigger/function`**\n",
    "\n",
    "`----------------------------------------------`\n",
    "\n",
    "+ **`Performance Issues:`**\n",
    "\n",
    "When using the **`ON CONFLICT`** you are able to do `pre-checks to find conflicts` before insertion. If the checks pass then insert is performed. Otherswise, the attempt is deleted and moves on.\n",
    "\n",
    "+ This is a step in the right direction because you are avoiding overhead resources creating a heap that is later deleted. When this occurs you are creating dead tuples which create wasted storage space.\n",
    "\n",
    "\n",
    "+ `BLOAT:` when we are scanning tables and updating old entries with new ones we create dead tuples that occur from the deletion of an old entry \"tuple\". Overtime this will affect speed and storage space if it is not controled\n",
    "\n",
    "`----------------------------------------------`\n",
    "\n",
    "# <font color=red>Important Side Note:</font> *Batch Inserts* use `COPY` \n",
    "\n",
    "+ Consider using a `Temp Table` if you are interested in doing operations to `remove duplicates`, restraints or similar for speed ups and retain data without/prevent lose\n",
    "\n",
    "`----------------------------------------------`\n",
    "\n",
    "**`ON CONFLICT DO NOTHING`**\n",
    "\n",
    "+ When a duplicate row is trying to be added it will be ignored and NOT create `bloat` such as `dead rows` of data and wasted space while creating an `insert`.\n",
    "    + A `dead tuple` is creating additional overhead in the form of storage space. Avoiding this will help in the long run.\n",
    "+ `Consideration for UPDATES` you are deleting an old row and inserting a new row creating a dead row from the old deleted row during update which can cause bloat.\n",
    "\n",
    "`-----------------------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a160572c",
   "metadata": {},
   "source": [
    "# `UNIQUE INDEX` or `Constraint`\n",
    "\n",
    "+ If you decide to use this `constraint` understand that you can apply it to\n",
    "`1 or more columns` but you `cannot have rows with repeating information` and `NULL` values are viewed as `not equal`.\n",
    "    + There is a provision to account for the `Null` values if you need to `NULLS NOT DISTINCT` and treats as equal\n",
    "    + When using with `multiple columns` rows with combined values cannot be repeated\n",
    "+ `When to USE:`\n",
    "    + If you have `columns` that are `called often` in queries\n",
    "    + When a `Where` clause or `Join` are performed often for these columns\n",
    "+ `Avoid IF:`\n",
    "    + Columns are `UPDATED` often\n",
    "    + Small tables size\n",
    "    + Columns `Not used often`\n",
    "    + This DOES create overhead for `INSERT` or `UPDATES` so take into consideration\n",
    "\n",
    "+ `Unique Index` creates unique indexed columns\n",
    "    + While `Unique Constraint` ensures that duplicates are not created and makes a unique index as well\n",
    "\n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "**`Create Table with Unique Constraint`**\n",
    "\n",
    "\n",
    "`CREATE TABLE Customer_orders ( Transaction_id integer PRIMARY KEY,\n",
    "User_name VARCHAR(255) NOT NULL,\n",
    "Order_date date,\n",
    "Quantity_Items integer,\n",
    "Order_notes varchar(200),\n",
    "CONSTRAINT User_unique UNIQUE (User_name)\n",
    ");`\n",
    "\n",
    "*If we were to add multiple columns just add a comma inside the parenthesis and your additional columns*\n",
    "\n",
    "`-------------`\n",
    "\n",
    "**`Alter Table:`** Use this if you have an existing table\n",
    "\n",
    "\n",
    "`ALTER TABLE Customer_orders\n",
    "ADD CONSTRAINT User_unique UNIQUE (User_name);`\n",
    "\n",
    "`-------------`\n",
    "\n",
    "**`Delete \"Drop\" Constraint`**\n",
    "\n",
    "`ALTER TABLE Customer_orders\n",
    "DROP CONSTRAINT User_unique;`\n",
    "\n",
    "`-------------`\n",
    "\n",
    "**`Unique Index`**\n",
    "\n",
    "\n",
    "`CREATE UNIQUE INDEX idx_transaction_id\n",
    "ON Customer_orders(Transaction_id);`\n",
    "\n",
    "`-------------`\n",
    "\n",
    "# **`Show Indexes for Current Database`**\n",
    "\n",
    "\n",
    "`SELECT\n",
    "tablename,\n",
    "indexname,\n",
    "indexdef\n",
    "FROM\n",
    "pg_indexes\n",
    "WHERE\n",
    "schemaname = 'public'\n",
    "ORDER BY\n",
    "tablename,\n",
    "indexname;`\n",
    "\n",
    "`---------Output--------------`\n",
    "\n",
    "\n",
    "|    tablename    |      indexname     |                                              indexdef                                              |\n",
    "|:---------------:|:------------------:|:--------------------------------------------------------------------------------------------------:|\n",
    "| Refunds         | Refund_id_pkey     | CREATE UNIQUE INDEX Refund_id_pkey ON public.Refunds USING btree(Refund_id)                        |\n",
    "| Customer_Orders | Customer_orders_pk |   CREATE UNIQUE INDEX Customer_orders_pkey ON public.Customer_orders USING btree(Transaction_id)   |\n",
    "| Customer_Orders |   User_unique_key  | CREATE UNIQUE INDEX Customer_orders_User_name_key ON public.Customer_orders USING btree(User_name) |\n",
    "| Pizza_Orders    | User_name_key      | CREATE UNIQUE INDEX Pizza_orders_User_name_key ON public.Pizza_orders USING btree(User_name)       |\n",
    "\n",
    "\n",
    "\n",
    "# **`Show Indexes for Current Table`**\n",
    "\n",
    "\n",
    "`SELECT\n",
    "indexname,\n",
    "indexdef\n",
    "FROM\n",
    "pg_indexes \n",
    "WHERE\n",
    "tablename = 'Customer_orders';`\n",
    "\n",
    "`------Output-------`\n",
    "\n",
    "\n",
    "|    tablename    |      indexname     |                                              indexdef                                              |\n",
    "|:---------------:|:------------------:|:--------------------------------------------------------------------------------------------------:|\n",
    "| Customer_Orders | Customer_orders_pk |  CREATE UNIQUE INDEX Customer_orders_pkey ON public.Customer_orders USING btree(Transaction_id)    |\n",
    "| Customer_Orders |   User_unique_key  | CREATE UNIQUE INDEX Customer_orders_User_name_key ON public.Customer_orders USING btree(User_name) |\n",
    "\n",
    "`------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bbf809",
   "metadata": {},
   "source": [
    "# `Triggers & When To USE/NOT Use`\n",
    "\n",
    "Think of this as a callback based on operations performed on given events. This will trigger a function to run which in turn calls the trigger to act.\n",
    "\n",
    "+ If this was a `bulk insert (avoid triggers)` but a single line insertion may not make a big deal up to a point.\n",
    "\n",
    "Triggers can do automatic tasks such as `INSERT, DELETE, UPDATE` for example. \n",
    "+ Consider two options:\n",
    "    + are you doing row-by-row\n",
    "    + transaction based irrespect of number of rows\n",
    "    \n",
    "`The trigger will be specified: before, after or inplace of some operation!`\n",
    "\n",
    "Here are a few examples of what you can do with them: \n",
    "+ `Scheduling a task`\n",
    "+ `Possible Error Handling`\n",
    "+ `Check for changes in your data`\n",
    "+ `Logging or auditing`\n",
    "    + Ex.) Consider if you were auditing users and their actions for some instance\n",
    "    + Ex.) Auditing private information for users which will not be shown due to sensitivity/restrictions\n",
    "        + Like: login/user information, time something may occur and you capture this, or database related information that is private\n",
    "+ `Validating tasks`\n",
    "\n",
    "`---------------------------`\n",
    "\n",
    "+ **Good use case:** consider speeding up an update with a trigger where you use a temp table for operations here is a starter to think about: [In Memory Options as a start](https://www.enterprisedb.com/postgres-tutorials/how-tune-postgresql-memory) | [secondary resource](https://postgrespro.com/docs/enterprise/10/in-memory) | [AWS PSQL In Memory](https://aws.amazon.com/blogs/database/introducing-optimized-reads-for-amazon-rds-for-postgresql/)\n",
    "\n",
    "`---------------------------`\n",
    "\n",
    "**`Downsides or when to Avoid:`**\n",
    "\n",
    "+ Possible `performance` slowdowns such as server loads\n",
    "+ If you have a high volume of data being used it would not be recommended\n",
    "+ `Debugging` can be an issue, for example client side applications won't see the trigger \n",
    "+ `Stored precedures within trigger`, try to `avoid` if possible\n",
    "+ be leery of cross-database (especially if maintenance is due) or less cross-server triggers due to speed concerns\n",
    "+ `Triggers firing more triggers`, try to `stay away` from this\n",
    "+ `Recurssive triggers` turned (debug or performance issues)\n",
    "    + `Try to reduce the number of write operations!`\n",
    "+ `Iterating` when you are doing a row-by-row read or comparison this drastically reduces performance such as if you are using `WHILE or CURSORs`\n",
    "\n",
    "**`PSQL Slight Differences from MySQL Triggers:`**\n",
    "\n",
    "+ Truncating triggers\n",
    "+ Trigger function is needed to call the actual trigger\n",
    "+ Normal operations: *`Create Trigger`*, *`Drop Trigger`*, *`Alter Trigger`*, *`Disable/Enable Trigger`*\n",
    "\n",
    "**A Few Take Aways:**\n",
    "\n",
    "+ A user needs permission/privilege `TRIGGER` and `EXECUTE` to use\n",
    "+ `pg_catalogue` will allow you to check all triggers for a given database\n",
    "+ If you are creating multiple triggers that fire on same object they will run in alphabetical order\n",
    "\n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "There are a few steps to set this up:\n",
    "\n",
    "+ Create a function without parameters\n",
    "    + Create the trigger\n",
    "        + 1.) create a trigger name\n",
    "        + 2.) `BEFORE` or `AFTER` an event occurs\n",
    "        + 3.) what are you doing? `INSERT`, `DELETE`, `UPDATE`, `TRUNCATE`\n",
    "        + 4.) call the `table_name` after you use `ON` keyword\n",
    "        + 5.) Is this a row-by-row reference: `FOR EACH ROW` or a statement `FOR EACH STATEMENT`\n",
    "        + 6.) call the `EXECUTE PROCEDURE` and then put your trigger_name afterward\n",
    "\n",
    "\n",
    "# **`Create Trigger`**\n",
    "\n",
    "**Assume we have this Table**\n",
    "\n",
    "`CREATE TABLE Customer_orders ( Transaction_id integer PRIMARY KEY,\n",
    "User_name VARCHAR(60) UNIQUE NULLS NOT DISTINCT,\n",
    "logged_into_acct TIMESTAMP(6) NOT NULL,\n",
    "Order_date date,\n",
    "Quantity_Items integer,\n",
    "Order_notes varchar(200),\n",
    "email varchar(60) UNIQUE NULLS NOT DISTINCT,\n",
    "order_placed_at TIMESTAMP(6) NOT NULL\n",
    ");`\n",
    "\n",
    "\n",
    "**Next, create our function to call trigger**\n",
    "+ I decided to make this regarding changing an email for a customer\n",
    "\n",
    "`--------------------------------------------------`\n",
    "\n",
    "CREATE OR REPLACE FUNCTION log_User_Email_changes()\n",
    "\n",
    "    RETURNS TRIGGER\n",
    "  \n",
    "    LANGUAGE PLPGSQL\n",
    "\n",
    "    AS\n",
    "\n",
    "$$\n",
    "\n",
    "BEGIN\n",
    "\n",
    "    IF NEW.email <> OLD.email THEN\n",
    "\n",
    "        INSERT INTO email_change_logs(email,User_name,email_changed_at)\n",
    "\n",
    "        VALUES(OLD.email,OLD.User_name,now());\n",
    "\n",
    "    END IF;\n",
    "\n",
    "    RETURN NEW;\n",
    "END;\n",
    "\n",
    "$$\n",
    "\n",
    "`-------------------------------------------------`\n",
    "\n",
    "**Create the New Table:** *`email_change_logs`*\n",
    "\n",
    "CREATE TABLE email_change_logs (\n",
    "   email varchar(60) UNIQUE NULLS NOT DISTINCT,\n",
    "   User_name VARCHAR(60),\n",
    "   email_changed_at TIMESTAMP(6) NOT NULL\n",
    ");\n",
    "\n",
    "`--------------------------------------------------`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ce555",
   "metadata": {},
   "source": [
    "\n",
    "# **`Finally, the trigger itself`**\n",
    "\n",
    "\n",
    "CREATE TRIGGER log_User_Email_changes\n",
    "\n",
    "    AFTER INSERT\n",
    "\n",
    "    ON \"Customer_orders\"\n",
    "\n",
    "    FOR EACH ROW\n",
    "\n",
    "    EXECUTE PROCEDURE log_User_Email_changes();\n",
    "    \n",
    "`--------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e9eb2",
   "metadata": {},
   "source": [
    "# *Customer_orders Table INSERT then call our Trigger?*\n",
    "\n",
    "\n",
    "`--------------------------------------------------`\n",
    "\n",
    "**`INSERT INTO Customer_orders(Transaction_id,User_name,logged_into_acct,Order_date,Quantity_Items,\n",
    "Order_notes, email, order_placed_at) VALUES\n",
    "( 111,'HorseTooth_John05','2023-08-03 12:50:50','2023-08-02 11:00:05', 0,'',\n",
    "'johnyBeebop@mail.com','2023-08-03 22:20:10')`**\n",
    "\n",
    "`--------------------------------------------------`\n",
    "\n",
    "\n",
    "**`SELECT * FROM Customer_orders`**\n",
    "\n",
    "| Transaction_id |           User_name          |    logged_into_acct   |       Order_date      | Quantity_Items | Order_notes |          email         | email_changed_at      |\n",
    "|:--------------:|:----------------------------:|:---------------------:|:---------------------:|:--------------:|:-----------:|:----------------------:|-----------------------|\n",
    "|       111      | 'HorseTooth_John05' | '2023-08-03 12:50:50' | '2023-08-02 11:00:05' |        2       |      ''     | 'johnyBeebop@mail.com' | '2023-08-03 22:20:10' |\n",
    "\n",
    "`--------------------------------------------------`\n",
    "\n",
    "**`SELECT * FROM employee_insert_trigger`**\n",
    "\n",
    "|             email            |      User_name      | email_changed_at      |\n",
    "|:----------------------------:|:-------------------:|-----------------------|\n",
    "| 'HorseTooth_Cowboy@mail.com' | 'HorseTooth_John05' | '2023-08-03 22:20:10' |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4135ac80",
   "metadata": {},
   "source": [
    "`--------------------------------------------------`\n",
    "\n",
    "# **`Update`**\n",
    "\n",
    "\n",
    "CREATE TRIGGER email_update_users\n",
    "\n",
    " BEFORE UPDATE\n",
    "\n",
    " ON \"User_name\"\n",
    "\n",
    " FOR EACH ROW\n",
    "\n",
    "EXECUTE PROCEDURE log_User_Email_changes();\n",
    "\n",
    "\n",
    "\n",
    "`--------------------------------------------------`\n",
    "\n",
    "\n",
    "# **`Delete Event Trigger`**\n",
    "\n",
    "CREATE TRIGGER customer_Data_delete_trigger\n",
    "\n",
    "    AFTER DELETE\n",
    "\n",
    "    ON \"Customer_orders\"\n",
    "\n",
    "    FOR EACH ROW\n",
    "\n",
    "EXECUTE PROCEDURE after_envent_delete_fcn();\n",
    "\n",
    "*`after_delete_fcn() this would be a function you created with whatever you want to do`*\n",
    "\n",
    "`--------------------------------------------------`\n",
    "\n",
    "\n",
    "# **`Drop`**\n",
    "\n",
    "`drop trigger log_User_Email_changes on \"Customer_orders\";`\n",
    "\n",
    "`--------------------------------------------------`\n",
    "\n",
    "# `See All Triggers for a Database`\n",
    "\n",
    "`SELECT your_DB_name FROM pg_trigger;`\n",
    "\n",
    "\n",
    "`--------------------------------------------------`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe687c8a",
   "metadata": {},
   "source": [
    "# `Distinct:`\n",
    "\n",
    "+ Find `Duplicate` rows based on *`1 column or more`* using a `SELECT` statement.\n",
    "\n",
    "+ You can use `Distinct ON` in your query to take the first occurence of a duplicate\n",
    "\n",
    "+ `When NOT TO USE:`\n",
    "    + `Where` clause\n",
    "    + More than 1 distinct key word search in a given SELECT statement such as calling DISTINCT col_1, DISTINCT col_2\n",
    "    + `Group By`\n",
    "    \n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "\n",
    "`Customer_acct ( first_name VARCHAR(60) NOT NULL, last_name VARCHAR(60) NOT NULL, User_name VARCHAR(60) UNIQUE NULLS NOT DISTINCT, Acct_notes varchar(200), email varchar(60) UNIQUE NULLS NOT DISTINCT, login_time TIMESTAMP(6) NOT NULL, Origin_country_abrev CHAR(3) NOT NULL, birth_yr CHAR(4) );`\n",
    "\n",
    "\n",
    "| first_name |  last_name |     User_name     |        Acct_notes        |            email            | login_time       | Origin_country_abrev | birth_yr |\n",
    "|:----------:|:----------:|:-----------------:|:------------------------:|:---------------------------:|------------------|:--------------------:|:--------:|\n",
    "|   \"Jason\"  | \"Shingles\" |   \"J_Shingles07\"  |            ''            |       \"js07@somemail\"       | 2023-08-04 12:50 |         \"CA\"         |   1975   |\n",
    "|   \"John\"   |  \"Rivers\"  | \"whoseYoDaddy_11\" |      '3 login fails'     |      \"whoseYodd@jjmail\"     | 2023-08-04 10:00 |         \"USA\"        |   1999   |\n",
    "|   \"John\"   |   \"Cash\"   |   \"JCash_bruh99\"  |       'new account'      |   \"johnnyCsh_money@mymail\"  | 2023-08-03 06:33 |         \"UK\"         |   2001   |\n",
    "|   \"Ricky\"  |  \"Dovers\"  |   \"rDov_wham11\"   |    'user name updated'   |     \"ricky_d69@yourmail\"    | 2023-05-03 09:50 |         \"MX\"         |   1989   |\n",
    "|   \"Mike\"   |   \"Smith\"  |     \"msmith21\"    | 'origin country updated' | \"mikeySmithy001@travelmail\" | 2022-08-03 12:50 |         \"USA\"        |   2002   |\n",
    "|   \"Chris\"  |   \"Smith\"  |   \"CSmith_0906\"   |            ''            |   \"whodat191@creycraymail\"  | 2023-08-03 12:50 |         \"CA\"         |   1979   |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**`One Column`**\n",
    "\n",
    "\n",
    "\n",
    "**`More Than 1 Column`**\n",
    "\n",
    "\n",
    "\n",
    "`------------------------------------------------`\n",
    "    \n",
    "    \n",
    "# `Distinct ON:`\n",
    "\n",
    "This is similar to above except it is useful when you want a specific `ordering` of your non-duplicated entries. Such as if you have `multiple entries but choose the first occurence`.\n",
    "+ If for instance you have multiple columns but non-unique rows this can be a beneficial use case and output the first occurence of duplicates\n",
    "    \n",
    "+ The theory here from what I read was a `temp file` is created during the `group by` as well as other tasks such as reads/writes which create overhead adding between 5-10% decreases in speed. I cannot say this is true but here is a read which suggests this [psql distinct vs group by speed](https://nolongerset.com/distinct-vs-group-by-jet-speed-test/)\n",
    "    \n",
    "+ If using this in conjunction with `Group By` you will need both to have same ordering of columns\n",
    "\n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "`More Than 1 column but 1 Table`\n",
    "\n",
    "\n",
    "\n",
    "`More Than 1 column but 2 Tables`\n",
    "\n",
    "\n",
    "`------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc5df0e",
   "metadata": {},
   "source": [
    "# `Group By:`\n",
    "\n",
    "+ Aggregate usage and flexibility\n",
    "+ Should be `faster than` *distinct* `but check if this is true`. There are a lot considerations with this though!\n",
    "    + The problem is the differing opinions and information to read!\n",
    "    \n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "\n",
    "\n",
    "`------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbca900",
   "metadata": {},
   "source": [
    "#  `&&` Overlap Operator Comparing Arrays\n",
    "\n",
    "+ The output will show `(T/F)` and `number of duplicate` entries\n",
    "\n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "\n",
    "\n",
    "`------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ebccb",
   "metadata": {},
   "source": [
    "# `Inner Join`\n",
    "\n",
    "+ Using two or more tables to construct a join based on something common between both tables such as a primary key to remove duplicate rows. \n",
    "\n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "\n",
    "\n",
    "`------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1dd8cf",
   "metadata": {},
   "source": [
    "**`If you want coded PSQL or Psycopg let me know for a future video`**\n",
    "\n",
    "Also, I am considering other topics of PSQL future videos so stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd902d6",
   "metadata": {},
   "source": [
    "# Like, Share & <font color=red>SUB</font>scribe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7d1f2",
   "metadata": {},
   "source": [
    "# `Citations`\n",
    "\n",
    "# ◔̯◔\n",
    "\n",
    "https://tomcam.github.io/postgres/\n",
    "\n",
    "https://aws.amazon.com/blogs/database/hidden-dangers-of-duplicate-key-violations-in-postgresql-and-how-to-avoid-them/\n",
    "\n",
    "https://www.freecodecamp.org/news/how-to-remove-duplicate-data-in-sql/#:~:text=One%20of%20the%20easiest%20ways,values%20from%20a%20particular%20column\n",
    "\n",
    "https://subscription.packtpub.com/book/data/9781803248974/5/ch05lvl1sec63/preventing-duplicate-rows\n",
    "\n",
    "https://stackoverflow.com/questions/67616081/preventing-insert-on-duplicate-values-postgres\n",
    "\n",
    "https://stackoverflow.com/questions/1109061/insert-on-duplicate-update-in-postgresql/30118648#30118648\n",
    "\n",
    "https://www.postgresql.org/docs/current/btree-gist.html\n",
    "\n",
    "https://codingsight.com/sql-insert-into-select-5-easy-ways-to-handle-duplicates/\n",
    "\n",
    "https://learn.microsoft.com/en-us/troubleshoot/sql/database-engine/development/remove-duplicate-rows-sql-server-tab\n",
    "\n",
    "https://www.mongodb.com/community/forums/t/batch-insert-upsert-avoiding-duplicates/163725 (mongodb)\n",
    "\n",
    "https://stackoverflow.com/questions/53722405/how-to-insert-bulk-rows-and-ignore-duplicates-in-postgresql-9-3\n",
    "\n",
    "https://www.psycopg.org/psycopg3/docs/advanced/async.html\n",
    "\n",
    "https://www.postgresql.org/docs/current/ddl-generated-columns.html\n",
    "\n",
    "https://www.appsloveworld.com/postgresql/100/58/how-to-do-a-bulk-insert-while-avoiding-duplicates-in-postgresql\n",
    "\n",
    "https://alibaba-cloud.medium.com/use-of-the-postgresql-upsert-insert-on-conflict-do-function-f366ac8afd52 (good examples)\n",
    "\n",
    "https://www.postgresqltutorial.com/postgresql-tutorial/how-to-delete-duplicate-rows-in-postgresql/\n",
    "\n",
    "https://www.delftstack.com/howto/postgres/postgresql-insert-on-duplicate-update/ (cool read, look at RACE ex.)\n",
    "\n",
    "https://www.geeksforgeeks.org/multiple-indexes-vs-multi-column-indexes/ (Index info)\n",
    "\n",
    "https://www.postgresql.org/docs/current/indexes-multicolumn.html (Index key points)\n",
    "\n",
    "https://devcenter.heroku.com/articles/postgresql-indexes (Index info 2)\n",
    "\n",
    "https://rbranson.medium.com/10-things-i-hate-about-postgresql-20dbab8c2791 (issues with PSQL)\n",
    "\n",
    "https://www.enterprisedb.com/postgres-tutorials/how-select-distinct-values-query-results-postgresql\n",
    "\n",
    "https://www.postgresqltutorial.com/postgresql-tutorial/postgresql-select-distinct/\n",
    "\n",
    "https://nolongerset.com/distinct-vs-group-by-jet-speed-test/\n",
    "\n",
    "https://database.guide/2-ways-to-delete-duplicate-rows-in-postgresql-ignoring-the-primary-key/ (good examples)\n",
    "\n",
    "https://www.c-sharpcorner.com/article/different-ways-to-find-and-delete-duplicate-rows-from-a-table-in-sql-server/\n",
    "\n",
    "https://copyprogramming.com/howto/sql-delete-duplicate-combined-rows-in-postgresql (interesting read)\n",
    "\n",
    "https://medium.com/flatiron-engineering/uniqueness-in-postgresql-constraints-versus-indexes-4cf957a472fd\n",
    "\n",
    "https://www.geeksforgeeks.org/postgresql-list-indexes/\n",
    "\n",
    "`Triggers`\n",
    "\n",
    "https://stackoverflow.com/questions/460316/are-database-triggers-necessary#:~:text=In%20this%20case%20triggers%20cause,that%20triggers%20are%20indeed%20harmful\n",
    "\n",
    "https://stackoverflow.com/questions/460316/are-database-triggers-necessary#:~:text=In%20this%20case%20triggers%20cause,that%20triggers%20are%20indeed%20harmful\n",
    "\n",
    "https://www.tutorialspoint.com/What-are-the-advantages-disadvantages-and-restrictions-of-using-MySQL-triggers\n",
    "\n",
    "https://www.red-gate.com/simple-talk/databases/sql-server/database-administration-sql-server/sql-server-triggers-good-scary/ (good read!)\n",
    "\n",
    "https://www.sqlservercentral.com/articles/postgresql-triggers-part-1\n",
    "\n",
    "https://www.enterprisedb.com/postgres-tutorials/everything-you-need-know-about-postgresql-triggers\n",
    "\n",
    "https://www.postgresqltutorial.com/postgresql-triggers/creating-first-trigger-postgresql/\n",
    "\n",
    "https://www.techonthenet.com/postgresql/unique.php\n",
    "\n",
    "`Bulk Insert`\n",
    "\n",
    "https://www.enterprisedb.com/blog/7-best-practice-tips-postgresql-bulk-data-loading\n",
    "\n",
    "https://www.commandprompt.com/education/how-to-insert-bulk-data-in-postgresql/\n",
    "\n",
    "https://www.sqlshack.com/working-with-line-numbers-and-errors-using-bulk-insert/\n",
    "\n",
    "https://www.2ndquadrant.com/en/blog/7-best-practice-tips-for-postgresql-bulk-data-loading/\n",
    "\n",
    "https://www.cockroachlabs.com/docs/stable/performance-best-practices-overview\n",
    "\n",
    "https://www.highgo.ca/2020/12/08/bulk-loading-into-postgresql-options-and-comparison/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225a006a",
   "metadata": {},
   "source": [
    "# `Avoiding Duplicate Entries PSQL`\n",
    "\n",
    "# <font color=red>Mr Fugu Data Science</font>\n",
    "\n",
    "# (◕‿◕✿)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d1072",
   "metadata": {},
   "source": [
    "# `Lets start from the` <font color=red>INSERT</font> `statement: you have 2 options`\n",
    "\n",
    "When you decide to insert either a *batch file, numberous CSV files or just a single line* there are distinctions to consider for `avoiding duplicate entries`\n",
    "\n",
    "`We have a few options depending on what scenario you fall into:`\n",
    "\n",
    "`--------------------------------------------`\n",
    "\n",
    "**`1. )`** Is this a situation where you want to find duplicate entries you **`already have in an existing DB?`**\n",
    "\n",
    "+ **`DISTINCT`** keyword\n",
    "\n",
    "+ **`GROUPBY`** clause\n",
    "\n",
    "+ **`INNER JOIN`** two or more tables\n",
    "\n",
    "+ **`&&`** which is an `overlap` operator\n",
    "\n",
    "**`2. )`** Are you trying to **`prevent creating duplicate entries before adding`** to a database?\n",
    "\n",
    "\n",
    "\n",
    "+ **`ON CONFLICT DO NOTHING`**\n",
    "\n",
    "+ **`UNIQUE`** constraint\n",
    "\n",
    "+ Option to build a **`trigger/function`**\n",
    "\n",
    "`----------------------------------------------`\n",
    "\n",
    "+ **`Performance Issues:`**\n",
    "\n",
    "When using the `ON CONFLICT` you are able to do pre-checks to find conflicts before insertion. If the checks pass then insert is performed otherswise, the attempt is deleted and moves on.\n",
    "\n",
    "+ This is a step in the right direction because you are avoiding overhead resources creating a heap that is later deleted. When this occurs you are creating dead tuples which create wasted storage space.\n",
    "\n",
    "\n",
    "+ `BLOAT:` when we are scanning tables and updating old entries with new ones we create dead tuples that occur from the deletion of an old entry \"tuple\". Overtime this will affect speed and storage space if it is not controled\n",
    "\n",
    "`----------------------------------------------`\n",
    "\n",
    "# <font color=red>Important Side Note:</font> `Batch Inserts use COPY` \n",
    "\n",
    "+ Also, consider using a `Temp Table` if you are interested in doing operations to remove duplicates, restraints or similar for speed ups and retain data without/prevent lose\n",
    "\n",
    "`----------------------------------------------`\n",
    "\n",
    "**`ON CONFLICT DO NOTHING`**\n",
    "\n",
    "+ When a duplicate row is trying to be added it will be ignored and NOT create `bloat` such as dead rows of data and wasted space while creating an `insert`.\n",
    "+ `Consideration for UPDATES` you are deleting an old row and inserting a new row creating a dead row from the old deleted row during update which can cause bloat.\n",
    "\n",
    "`-----------------------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a160572c",
   "metadata": {},
   "source": [
    "# `UNIQUE INDEX` or `Constraint`\n",
    "\n",
    "+ If you decide to use this `constraint` understand that you can apply it to `1 or more columns` but you `cannot have rows with `repeating information` and `NULL` values are viewed as `not equal`.\n",
    "    + There is a provision to account for the `Null` values if you need to `NULLS NOT DISTINCT` and treats as equal\n",
    "+ `When To USE:`\n",
    "    + If you have columns that are called often in queries\n",
    "    + When a `Where` clause or `Join` are performed often for these columns\n",
    "+ `Avoid IF:`\n",
    "    + These columns are `UPDATED` often\n",
    "    + Small tables sizes\n",
    "    + Columns `Not used often`\n",
    "    + They DO take up overhead for your `INSERT` or `UPDATES` so take into consideration\n",
    "\n",
    "+ `Unique Index` creates unique indexed columns\n",
    "    + While `Unique Constraint` ensures that duplicates are not created and under the hood makes a unique index as well\n",
    "\n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "**`Create Table`**\n",
    "\n",
    "**`Create Unique Constraint`**\n",
    "\n",
    "**`Alter Table`**\n",
    "\n",
    "**`Delete Constraint`**\n",
    "\n",
    "**`Unique Index`**\n",
    "\n",
    "`------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bbf809",
   "metadata": {},
   "source": [
    "# `Triggers & When To USE/NOT Use`\n",
    "\n",
    "This I think from what I read will depend on the type of insertion you are doing since the trigger is doing a row-wise comparision. \n",
    "\n",
    "+ So if this was a `bulk insert (avoid triggers) but a single line insertion may not make a big deal`.\n",
    "\n",
    "You will use a trigger to do automatic tasks such as `INSERT, DELETE, UPDATE` for example. \n",
    "+ Consider two options:\n",
    "    + are you doing row-by-row\n",
    "    + transaction based irrespect of number of rows\n",
    "\n",
    "Here are a few examples of what you can do with them: \n",
    "+ `Scheduling a task`\n",
    "+ `Possible Error Handling`\n",
    "+ `Check for changes in your data`\n",
    "+ `Logigng or auditing such as a schema`\n",
    "    + Ex.) Consider if you were auditing users and their actions for some instance\n",
    "    + Ex.) Auditing private information for users which will not be shown due to sensitivity/restrictions\n",
    "        + Like: login/user information, time something may occur and you capture this, or database related information that is private\n",
    "+ `Validating tasks`\n",
    "\n",
    "`---------------------------`\n",
    "\n",
    "+ **Good use case:** consider speeding up an update with a trigger where you use a temp table for operations here is a starter to think about: [In Memory Options as a start](https://www.enterprisedb.com/postgres-tutorials/how-tune-postgresql-memory) | [secondary resource](https://postgrespro.com/docs/enterprise/10/in-memory) | [AWS PSQL In Memory](https://aws.amazon.com/blogs/database/introducing-optimized-reads-for-amazon-rds-for-postgresql/)\n",
    "\n",
    "`---------------------------`\n",
    "\n",
    "**`Downsides or when to Avoid:`**\n",
    "\n",
    "+ Possible `performance` slowdowns such as server loads\n",
    "+ If you have a high volume of data being used it would not be recommended\n",
    "+ `Debugging` can be an issue, for example client side applications won't see the trigger \n",
    "+ `Stored precedures within trigger`, try to `avoid` if possible\n",
    "+ be leery of cross-database (especially if maintenance is due) or less cross-server triggers due to speed concerns\n",
    "+ `Triggers firing more triggers`, try to `stay away` from this\n",
    "+ `Recurssive triggers` turned (debug or performance issues)\n",
    "    + `Try to reduce the number of write operations!`\n",
    "+ `Iterating` when you are doing a row-by-row read or comparison this drastically reduces performance such as if you are using `WHILE or CURSORs`\n",
    "\n",
    "**`PSQL Slight Differences from MySQL Triggers:`**\n",
    "\n",
    "+ Truncating triggers\n",
    "+ Trigger function is needed to call the actual trigger\n",
    "+ Normal operations: *`Create Trigger`*, *`Drop Trigger`*, *`Alter Trigger`*, *`Disable/Enable Trigger`*\n",
    "\n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`------------------------------------------------`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe687c8a",
   "metadata": {},
   "source": [
    "# `Distinct:`\n",
    "\n",
    "+ Find `Duplicate` rows based on *`1 column or more`* using a `SELECT` statement.\n",
    "\n",
    "+ You can use `Distinct ON` in your query to take the first occurence of a duplicate\n",
    "\n",
    "+ `When NOT TO USE:`\n",
    "    + `Where` clause\n",
    "    + More than 1 distinct key word search\n",
    "    + `Group By`\n",
    "    \n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "\n",
    "\n",
    "`------------------------------------------------`\n",
    "    \n",
    "    \n",
    "# `Distinct ON:`\n",
    "\n",
    "This is similar to above except it is useful when you want a specific `ordering` of your non-duplicated entries. Such as if you have `multiple entries but choose the first occurence`.\n",
    "    \n",
    "+ The theory here from what I read was a `temp file` is created during the `group by` as well as other tasks such as reads/writes which create overhead adding between 5-10% decreases in speed. I cannot say this is true but here is a read which suggests this [psql distinct vs group by speed](https://nolongerset.com/distinct-vs-group-by-jet-speed-test/)\n",
    "    \n",
    "+ If using this in conjunction with `Group By` you will need both to have same ordering of columns\n",
    "\n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "\n",
    "\n",
    "`------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db96d2",
   "metadata": {},
   "source": [
    "# `Group By:`\n",
    "\n",
    "+ Aggregate usage and flexibility\n",
    "+ Should be `faster than` *distinct* `but check if this is true`. There are a lot considerations with this though!\n",
    "    + The problem is the differing opinions and information to read!\n",
    "    \n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "\n",
    "\n",
    "`------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae34efa4",
   "metadata": {},
   "source": [
    "#  `&&` Overlap Operator Comparing Arrays\n",
    "\n",
    "+ The output will show `(T/F)` and `number of duplicate` entries\n",
    "\n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "\n",
    "\n",
    "`------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7443318",
   "metadata": {},
   "source": [
    "# `Inner Join`\n",
    "\n",
    "+ Using two or more tables to construct a join based on something common between both tables such as a primary key to remove duplicate rows. \n",
    "\n",
    "`------------------------------------------------`\n",
    "\n",
    "# EX.)\n",
    "\n",
    "\n",
    "\n",
    "`------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509456d3",
   "metadata": {},
   "source": [
    "**`If you want coded PSQL or Psycopg let me know for a future video`**\n",
    "\n",
    "Also, I am considering other topics of PSQL future videos so stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd902d6",
   "metadata": {},
   "source": [
    "# Like, Share & <font color=red>SUB</font>scribe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7d1f2",
   "metadata": {},
   "source": [
    "# `Citations`\n",
    "\n",
    "# ◔̯◔\n",
    "\n",
    "https://tomcam.github.io/postgres/\n",
    "\n",
    "https://aws.amazon.com/blogs/database/hidden-dangers-of-duplicate-key-violations-in-postgresql-and-how-to-avoid-them/\n",
    "\n",
    "https://www.freecodecamp.org/news/how-to-remove-duplicate-data-in-sql/#:~:text=One%20of%20the%20easiest%20ways,values%20from%20a%20particular%20column\n",
    "\n",
    "https://subscription.packtpub.com/book/data/9781803248974/5/ch05lvl1sec63/preventing-duplicate-rows\n",
    "\n",
    "https://stackoverflow.com/questions/67616081/preventing-insert-on-duplicate-values-postgres\n",
    "\n",
    "https://stackoverflow.com/questions/1109061/insert-on-duplicate-update-in-postgresql/30118648#30118648\n",
    "\n",
    "https://www.postgresql.org/docs/current/btree-gist.html\n",
    "\n",
    "https://codingsight.com/sql-insert-into-select-5-easy-ways-to-handle-duplicates/\n",
    "\n",
    "https://learn.microsoft.com/en-us/troubleshoot/sql/database-engine/development/remove-duplicate-rows-sql-server-tab\n",
    "\n",
    "https://www.mongodb.com/community/forums/t/batch-insert-upsert-avoiding-duplicates/163725 (mongodb)\n",
    "\n",
    "https://stackoverflow.com/questions/53722405/how-to-insert-bulk-rows-and-ignore-duplicates-in-postgresql-9-3\n",
    "\n",
    "https://www.psycopg.org/psycopg3/docs/advanced/async.html\n",
    "\n",
    "https://www.postgresql.org/docs/current/ddl-generated-columns.html\n",
    "\n",
    "https://www.appsloveworld.com/postgresql/100/58/how-to-do-a-bulk-insert-while-avoiding-duplicates-in-postgresql\n",
    "\n",
    "https://alibaba-cloud.medium.com/use-of-the-postgresql-upsert-insert-on-conflict-do-function-f366ac8afd52 (good examples)\n",
    "\n",
    "https://www.postgresqltutorial.com/postgresql-tutorial/how-to-delete-duplicate-rows-in-postgresql/\n",
    "\n",
    "https://www.delftstack.com/howto/postgres/postgresql-insert-on-duplicate-update/ (cool read, look at RACE ex.)\n",
    "\n",
    "https://www.geeksforgeeks.org/multiple-indexes-vs-multi-column-indexes/ (Index info)\n",
    "\n",
    "https://www.postgresql.org/docs/current/indexes-multicolumn.html (Index key points)\n",
    "\n",
    "https://devcenter.heroku.com/articles/postgresql-indexes (Index info 2)\n",
    "\n",
    "https://rbranson.medium.com/10-things-i-hate-about-postgresql-20dbab8c2791 (issues with PSQL)\n",
    "\n",
    "https://www.enterprisedb.com/postgres-tutorials/how-select-distinct-values-query-results-postgresql\n",
    "\n",
    "https://www.postgresqltutorial.com/postgresql-tutorial/postgresql-select-distinct/\n",
    "\n",
    "https://nolongerset.com/distinct-vs-group-by-jet-speed-test/\n",
    "\n",
    "https://database.guide/2-ways-to-delete-duplicate-rows-in-postgresql-ignoring-the-primary-key/ (good examples)\n",
    "\n",
    "https://www.c-sharpcorner.com/article/different-ways-to-find-and-delete-duplicate-rows-from-a-table-in-sql-server/\n",
    "\n",
    "https://copyprogramming.com/howto/sql-delete-duplicate-combined-rows-in-postgresql (interesting read)\n",
    "\n",
    "https://medium.com/flatiron-engineering/uniqueness-in-postgresql-constraints-versus-indexes-4cf957a472fd\n",
    "\n",
    "`Triggers`\n",
    "\n",
    "https://stackoverflow.com/questions/460316/are-database-triggers-necessary#:~:text=In%20this%20case%20triggers%20cause,that%20triggers%20are%20indeed%20harmful\n",
    "\n",
    "https://stackoverflow.com/questions/460316/are-database-triggers-necessary#:~:text=In%20this%20case%20triggers%20cause,that%20triggers%20are%20indeed%20harmful\n",
    "\n",
    "https://www.tutorialspoint.com/What-are-the-advantages-disadvantages-and-restrictions-of-using-MySQL-triggers\n",
    "\n",
    "https://www.red-gate.com/simple-talk/databases/sql-server/database-administration-sql-server/sql-server-triggers-good-scary/ (good read!)\n",
    "\n",
    "https://www.sqlservercentral.com/articles/postgresql-triggers-part-1\n",
    "\n",
    "`Bulk Insert`\n",
    "\n",
    "https://www.enterprisedb.com/blog/7-best-practice-tips-postgresql-bulk-data-loading\n",
    "\n",
    "https://www.commandprompt.com/education/how-to-insert-bulk-data-in-postgresql/\n",
    "\n",
    "https://www.sqlshack.com/working-with-line-numbers-and-errors-using-bulk-insert/\n",
    "\n",
    "https://www.2ndquadrant.com/en/blog/7-best-practice-tips-for-postgresql-bulk-data-loading/\n",
    "\n",
    "https://www.cockroachlabs.com/docs/stable/performance-best-practices-overview\n",
    "\n",
    "https://www.highgo.ca/2020/12/08/bulk-loading-into-postgresql-options-and-comparison/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
